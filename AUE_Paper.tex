\documentclass[sigchi]{acmart}
\usepackage{graphicx}
\usepackage[nolist]{acronym}
\graphicspath{ {Plots/} }

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
	\providecommand\BibTeX{{%
			\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{none}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
	Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
	June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-9999-9/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

	\begin{acronym}
	\acro{gui}[GUI]{graphical user interface}
	\end{acronym}
	
	%%
	%% The "title" command has an optional parameter,
	%% allowing the author to define a "short title" to be used in page headers.
	\title{Different interaction modalities for smart home}
	
	%%
	%% The "author" command and its associated commands are used to define
	%% the authors and their affiliations.
	%% Of note is the shared affiliation of the first two authors, and the
	%% "authornote" and "authornotemark" commands
	%% used to denote shared contribution to the research.
	\author{Fabian Hoffmann}
	\affiliation{%
		\institution{University of Regensburg}
		\city{Regensburg}
		\state{Bavaria}
		\country{Germany}
	}
	\email{fabian.hoffmann@stud.uni-regensburg.de}
	
	\author{Miriam Ida Tyroller}
	\affiliation{%
		\institution{University of Regensburg}
		\city{Regensburg}
		\state{Bavaria}
		\country{Germany}
	}
	\email{miriam-ida.tyroller@stud.uni-regensburg.de}
	
	\author{Felix Wende}
	\affiliation{%
		\institution{University of Regensburg}
		\city{Regensburg}
		\state{Bavaria}
		\country{Germany}
	}
	\email{felix.wende@stud.uni-regensburg.de}
	
	%%
	%% By default, the full list of authors will be used in the page
	%% headers. Often, this list is too long, and will overlap
	%% other information printed in the page headers. This command allows
	%% the author to define a more concise list
	%% of authors' names for this purpose.
	%% \renewcommand{\shortauthors}{Trovato and Tobin, et al.}
	
	%%
	%% The abstract is a short summary of the work to be presented in the
	%% article.
	\begin{abstract}
		We present a study aimed to gain insight on users' perceptions and desires in the context of smart home interaction modalities. To achieve this, we conducted an elicitation study in which participants were asked to perform commands within a simulated smart home environment, facing three conditions: voice command, display interaction and mid-air gestures. Facing tasks of different areas in smart home that require user assistance, the participants suggested fitting commands and rated them on the grounds of goodness, ease, enjoyment and social acceptance, as well as their general preference of each modality. The collected measures allow us to present insights that can be used as possible future guidelines for smart home interaction modalities and future research in voice command, display interactions and mid-air gestures.
	\end{abstract}
	
	%%
	%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
	%% Please copy and paste the code instead of the example below.
	%%
	
	
	%%
	%% Keywords. The author(s) should pick words that accurately describe
	%% the work being presented. Separate the keywords with commas.
	\keywords{smart home, voice control, display control, mid-air gestures}
	
	
	%%
	%% This command processes the author and affiliation and title
	%% information and builds the first part of the formatted document.
	\maketitle
	
	\section{Introduction}
	Smart homes and its interaction modalities are widely spread ...  Existing work focuses on creating smart home interaction modalities considered desirable and enjoyable to use for its users, as developed by Hagensby Jensen et al. \cite{Jensen.2018}. ... This is caused by different obstacles, such as the interaction modalities being too expensive to implement or too complex to understand. Interacting with a smart home requires modalities that are easy to perform by its user, but also modalities that are easily coming to mind, with user enjoyment and social acceptance as variables also important to consider. Our goal is ... 
	
	\section{Approach and Methodology}
	We followed a similar approach as Dingler et al. \cite{Dingler.2018} by showing and explaining different smart home tasks to the participants and subsequently asking them to propose a voice command, a display interaction and a mid-air gesture, to fulfil the specific tasks in their preferred way. All eleven tasks are listed in section 'Tasks'. A within-subject design was chosen, so every participant gave suggestions for every modality and task. We used a latin-square on the order of the interaction modalities to reduce sequence effects \cite{.2017} and fatigue. The tasks were shown to the participants in  random order. We took video recordings of all sessions. We also collected feedback from participants through questionnaires, on preferences of interaction modalities for a specific task and on goodness, ease, enjoyment and social acceptance of their suggestions. The study was conducted in German.
	
	\subsection{Interaction Modalities}
	We compared three different types of interaction modalities. The already commonly used voice and display control, as well as the existing but lacking development technique of mid-air gestures. Therefore, we were able to collect insights on the existing modalities and additionally gain a new set of mid-air gestures.
	
	\subsection{Tasks}
	The smart home market can be divided into six different categories \cite{.c}. Those are \textit{home entertainment, smart household appliances, energy management, networking and control, comfort and light} and \textit{building security}. We excluded the category \textit{networking and control} for developing the tasks, because it does not include devices that can be controlled, but is rather the infrastructure of a smart home and would be responsible for the detection of performed commands. For all other categories we selected two common tasks \cite{.d} each, except for \textit{building security} three because of its bigger market share. All categories with their assigned tasks are listed in table \ref{tab:tasks}.
	\begin{table}[t]
		\caption{Categories with their assigned tasks}
		\label{tab:tasks}
		\begin{small}
			\begin{tabular}{p{0.35\columnwidth} p{0.6\columnwidth}} \toprule
				\textbf{Category}			& \textbf{Task} \\ \midrule
				Home Entertainment         	& \begin{itemize}
					\item[1.] Increase the volume of the music.
					\item[2.] Turn on the next TV channel.  
				\end{itemize} \\ \midrule
				Smart household appliances 	& \begin{itemize}
					\item[3.] Start multi-colored wash at 60 degree.
					\item[4.] Turn off the oven.   
				\end{itemize} \\ \midrule
				Energy Management          	& \begin{itemize}
					\item[5.] Increase the room temperature.   
					\item[6.] Open the shutters.    
				\end{itemize} \\ \midrule
				Comfort and light          	& \begin{itemize}
					\item[7.] Turn on the light.      
					\item[8.] Dim the light.          
				\end{itemize} \\ \midrule
				Building security          	& \begin{itemize}
					\item[9.] Close the window.     
					\item[10.] Lock the front door. 
					\item[11.]  Turn on the security camera.
				\end{itemize} \\ \bottomrule
				
			\end{tabular}
		\end{small}
	\end{table}
	
	%\begin{table*}[]
	%\caption{Categories with their assigned tasks}
	%\label{tab:tasks}
	%\begin{tabular}{|l|p{0.4\textwidth}|}
	%\hline
	%\textbf{Category}          & \textbf{Tasks} \\ \hline
	%Home Entertainment         & \begin{itemize}
	%							 \item Increase the volume of the music.
	%							 \item Turn on the next TV channel.  
	%							 \end{itemize} \\ \hline
	%Smart household appliances & \begin{itemize}
	%							 \item Start multi-colored wash at 60 degree.
	%							 \item Turn off the oven.   
	%							 \end{itemize} \\ \hline
	%Energy Management          & \begin{itemize}
	%							 \item Increase the room temperature.   
	%							 \item Open the shutters.    
	%							 \end{itemize} \\ \hline
	%Comfort and light          & \begin{itemize}
	%							 \item Turn on the light.      
	%							 \item Dim the light.          
	%							 \end{itemize} \\ \hline
	%Building security          & \begin{itemize}
	%							 \item Close the window.     
	%							 \item Lock the front door. 
	%							 \item  Turn on the security camera.
	%							 \end{itemize} \\ \hline
	%\end{tabular}
	%\end{table*}
	
	\subsection{Participants}
	A total of 13 participants (7 female) took part in the study with an average age of $33.5$ (SD = $15.1$). We recruited the participants through social networks and personal contacts. The participants were mostly students from different departments of the University of Regensburg and OTH Regensburg. All of them at least heard of smart homes before and are familiar with interaction through displays. According to the pre-questionnaire, ten participants are familiar with both voice control and display interaction to control other devices, but only one performed mid-air gestures for interaction yet. Seven participants own smart home devices like Google Home, Amazon Alexa, smart TVs or lamps and use them frequently. None of the participants owns a fully integrated smart home system.
	
	\subsection{Apparatus}
	The study was carried out in a quiet room. The different tasks were illustrated through pictures, which showed the state before and after issuing the command. Mid-air gestures, voice commands and comments of the participants were recorded by a mounted camera. Display interaction was documented through a sketch on paper. The whole setup is shown in Figure \ref{figure:Setup}. None of the interaction modalities were actually implemented.
	\begin{figure}
		\label{figure:Setup}			
		\centering
		\includegraphics[width=\columnwidth]{SetupZahlen}
		\caption{Setup with monitor to illustrate smart home tasks (1), camera for video recording (2) and the questionnaires (3)}
		\Description{Setup with monitor to illustrate smart home tasks (1), camera for video recording (2) and the questionnaires (3)}
	\end{figure}
	
	\subsection{Procedure}
	Before starting the session, the participants were asked to fill out a consent form and a demographic questionnaire. Then they had to fill out a questionnaire in terms of their previous knowledge and usage of smart home devices and the three interaction modalities. After that the tasks were presented to the participants in a random order. At first all tasks had to be fulfilled with a single interaction modality, then with the second and after that with the remaining modality. Additionally to the illustration through pictures, the tasks were explained verbally. The participants were allowed to talk, move and interact with a display in any way they wanted and were encouraged to explain their choices in a thinking-aloud approach. After each task the participants rated their specific suggestion on goodness, ease, enjoyment and social acceptance on four 7-point Likert scales. When all tasks were finished with each interaction modality the participants rated the three interaction modalities for each on 7-point Likert scales, on how good each modality is to perform the specific task. They were asked to do this independently of their own suggestions. At the end a semi-structured interview was conducted to explore the motivation of the participants for each choice and allow them to rate the different interaction modalities under the aspects of efficiency, simplicity, naturality, desirability and enjoyment. this is based on a similar approach in the elicitation study on foot gestures by Felberbaum et al. \cite{Felberbaum.2018}. The study took about an hour, for which the participants were compensated with sweets.
	
	\section{Results}
	With nine participants and eleven tasks, we collected for each of the three interaction modalities $143$ suggestions and in total $13*11*3=429$. Our results include the video recording, taxonomies for each interaction modality, user-defined sets of voice commands, display interactions and gestures, subjective ratings of the sets, qualitative observations and an assessment on the modalities for each task. 
	
	\subsection{Classification of Voice Commands}
	\subsubsection{Taxonomy of voice commands}
	The participants suggested $43$ unique voice commands. The authors manually classified each voice command along five dimensions: \textit{nature}, \textit{form}, \textit{flow}, \textit{context} and \textit{complexity}. Within each dimension are multiple categories, shown in Table \ref{tab:taxVoice}. We adopted the dimensions from Wobbrock \textit{et al.} \citep{Wobbrock.2009} and Ruiz \textit{et al.} \citep{Ruiz.2011} and adapted them to voice commands. 
	
	The \textit{nature} dimension compromises \textit{action} voice commands which state the action to perform. An example of this type of voice command is saying "increase temperature". \textit{State} voice commands describe the desired condition of a device. For example, a \textit{state} voice command is "cameras on" to start camera surveillance.
	
	The \textit{form} dimension describes how much words are used in the voice command and if they have the structure of a full sentence. A \textit{single word} command can be "next" to get to the next TV channel. \textit{Two words} voice commands mostly consist out of the mentioning of the device to be controlled and an action or state. \textit{More words} commands are similar to \textit{two words} but use additional filler words. Finally, voice commands that are correct sentences were classified with the category \textit{sentence}.
	
	The \textit{flow} dimension categorizes the voice commands, if response of a device occurs after or while the user acts. A voice command is\textit{discrete}, when device perform the command after the participant stopped talking. A \textit{continuous} voice command would be starting an action with a command and stop the ongoing action with another command.
	
	The \textit{context} dimension describes, if the voice command requires a specific context or can be performed independently. For example saying "turn off" to turn off the oven is \textit{in-context}, whereas "oven off" is considered \textit{no-context}.
	
	The \textit{complexity} dimension describes if the voice command consists out of a single or a composition of more voice commands. A \textit{compound} voice command can be decomposed into \textit{simple} voice commands.
	\begin{table}[t]
		\begin{center}
			\caption{Taxonomy of voice commands for smart home tasks}
			\label{tab:taxVoice}
			\begin{footnotesize}
				\begin{tabular}{p{0.2\columnwidth} p{0.2\columnwidth} p{0.5\columnwidth}} \toprule
					\multicolumn{3}{c}{\textit{Taxonomy of voice commands}} \\ \midrule
					\textbf{Nature}		& Action		& Voice command states the action to perform \\
										& State			& Voice command describes the desired condition \\ \midrule
					\textbf{Form} 		& Single word	& Voice command consists out of a single word \\
										& Two words		& Voice command consists out of two words \\
										& More words	& Voice command consists out of more words without sentence structure \\
										& Sentence		& Voice command uses sentence structure \\ \midrule
					\textbf{Flow}		& Discrete		& Response occurs \textit{after} the user acts \\ 
										& Continuous 	& Response occurs \textit{while} the user acts  \\ \midrule
					\textbf{Context}	& In-context	& Voice command requires specific context \\
										& No-context	& Voice command does not require specific context \\ \midrule
					\textbf{Complexity}	& Simple		& Voice command consists of a single voice command \\
										& Compound		& Voice command can be decomposed into simple voice commands  \\ \bottomrule
				\end{tabular}
			\end{footnotesize}
		\end{center}
	\end{table}
	\subsection{User-defined voice command set}
We collected a total of $143$ voice commands, which we used to create a user-defined voice command set for our specified tasks. For each task, we grouped identical voice commands together and the group with largest size was chosen to be the representative voice command for this corresponding task. To evaluate the degree of consensus among the participants, we computed the \textit{agreement score} $A_t$ (Equation \ref{eq:agreement score}), as proposed by Vatavu and Wobbrock \cite{Vatavu.2015}, for each task.
		\begin{equation}
			\label{eq:agreement score}
			A_t = \frac{|P_t|}{|P_t|-1} \sum_{P_i \subseteq P_t} \left(\frac{|P_i|}{|P_t|}\right)^2  - \frac{1}{|P_t|-1}
		\end{equation}
In equation \ref{eq:agreement score}, $t$ is a task in the set of all tasks $T$, $P_t$ is the set of suggested voice commands for $t$ and $P_i$ is a subset of identical voice commands from $P_t$. 
As an example of calculation of an agreement score, consider the task \textit{increase the volume of the music}. The task has four groups of identical voice commands with a size of $7$, $4$, $1$ and $1$. Therefore the agreement score for \textit{increase the volume of the music} is:
		\begin{equation}
			A = \frac{13}{12} \left(\left(\frac{7}{13}\right)^2 + \left(\frac{4}{13}\right)^2 + \left(\frac{1}{13}\right)^2 + \left(\frac{1}{13}\right)^2 \right)- \frac{1}{12} = 0.346
		\end{equation}
		Figure \ref{figure:ASvoice} illustrates the agreement scores for each task using voice commands. Participants had the least agreement on commands for the tasks \textit{start multi-colored wash at 60 degree} (Task 3) and \textit{increase the room temperature} (Task 5). This is attributable to the complexity of the tasks.
		\begin{figure*}
			\label{figure:ASvoice}			
			\centering
			\includegraphics[width=\textwidth]{AgreementVoice}
			\caption{Agreement scores for each task with voice commands}
			\Description{Agreement scores for each task with voice commands}
		\end{figure*}
	Since the study was conducted in German, we also provide the original version of the voice commands to prevent losses during translation. Table \ref{tab:UserVoice} shows the most common and second most common voice command and their frequency according to the different tasks.
	\begin{table}[t]
		\begin{center}
			\caption{User defined voice command set}
			\label{tab:UserVoice}
			\begin{footnotesize}				
				\begin{tabular}{p{0.1\columnwidth} p{0.3\columnwidth} p{0.3\columnwidth} p{0.2\columnwidth}} \toprule
					\textbf{Task}	& \textbf{German}			& \textbf{English}		& \textbf{Frequency} \\ \midrule
					1				& lauter					& louder				& 53.8\% \\ 
									& Musik lauter				& music louder			& 30.8\% \\ \midrule
					2				& Nächster Kanal			& next channel			& 46.2\% \\
									& weiter					& next					& 23.1\% \\ \midrule
					3				& ...						&						& \\ \bottomrule
				\end{tabular}
			\end{footnotesize}	
		\end{center}
	\end{table}	
	\subsection{Classification of Display Interaction}
	The participants suggested $61$ unique display interactions. Similar to the voice commands we manually classified each display interaction along three dimensions for \ac{gui} elements (\textit{form}, \textit{elements}, \textit{flow}) and for touch gestures along four dimensions (\textit{form}, \textit{nature}, \textit{binding}, \textit{flow}). Within each dimension are multiple categories, shown in Table \ref{tab:taxDisplayGUI} for \ac{gui} elements and for touch gestures in Table \ref{tab:taxDisplayTG}. The dimensions and categories for \ac{gui} elements were inspired by the work from Wobbrock \textit{et al.} \citep{Wobbrock.2009} and Ruiz \textit{et al.} \citep{Ruiz.2011}. As taxonomy for touch gestures we used Wobbrock \textit{et al.} \citep{Wobbrock.2009} taxonomy of surface gestures, witch is displayed in Table \ref{tab:taxDisplayTG}.
	\subsubsection{Taxonomy of display interaction (\ac{gui} elements):}
	ToDo: Beschreibung Taxonomy wie bei voice
	\begin{table}[t]
		\begin{center}
			\caption{Taxonomy of display interactions for smart home tasks (\ac{gui} elements)}
			\label{tab:taxDisplayGUI}
			\begin{footnotesize}				
				\begin{tabular}{p{0.2\columnwidth} p{0.2\columnwidth} p{0.5\columnwidth}} \toprule
					\multicolumn{3}{c}{\textit{Taxonomy of display interactions (\ac{gui} elements)}} \\ \midrule
					\textbf{Form}		& Direct Action				& Single interaction that directly leads to the action \\
										& Selection \& Confirmation	& Selection of the action and starting through another element\\ \midrule
					\textbf{Elements} 	& Single clickables			& The \ac{gui} includes one or more single clickables 
																	  (button, checkbox, etc.) \\
										& Slider					& The \ac{gui} includes one or more sliders \\
										& More words				& Voice command consists out of more words without sentence structure \\
										& Rotation					& The \ac{gui} includes one or more rotational elements \\ 
										& Text \& number entry		& The \ac{gui} includes one or more options to enter text or numbers \\
										& Symbolic					& The \ac{gui} includes one or more special symbolic elements\\ \midrule
					\textbf{Flow}		& Discrete		& Response occurs \textit{after} the user acts \\ 
										& Continuous 	& Response occurs \textit{while} the user acts  \\ \bottomrule
				\end{tabular}
			\end{footnotesize}	
		\end{center}
	\end{table}	
	\subsubsection{Taxonomy of display interaction (touch gestures):}	
	ToDo: Bschreibung Taxonomy wie bei voice
	\begin{table}[t]
		\begin{center}
			\caption{Taxonomy of display interactions for smart home tasks (touch gestures)}
			\label{tab:taxDisplayTG}
			\begin{footnotesize}
				\begin{tabular}{p{0.12\columnwidth} p{0.28\columnwidth} p{0.5\columnwidth}} \toprule
					\multicolumn{3}{c}{\textit{Taxonomy of display interactions (touch gestures)}} \\ \midrule
					\textbf{Nature}		& Symbolic		& Gesture visually depicts a symbol \\
										& Physical		& Gesture acts physically on objects \\ 
										& Metaphorical 	& Gesture indicates a metaphor \\ 
										& Abstract		& Gesture-referent mapping is arbitrary \\ \midrule
					\textbf{Form} 		& Static pose	& Hand pose is held in one location \\
										& Dynamic pose	& Hand pose changes in one location \\
										& Static pose and path 	& Hand pose is held as hand moves \\
										& Dynamic pose and path	& Hand pose changes as hand moves \\
										& One-point touch	& Static pose with one finger \\ 
										& One-point path	& Static pose and path with one Finger \\ \midrule
					\textbf{Binding}	& Object-centric 	& Location defined with respect to object features \\
										& World-dependent 	& Location defined with respect to world features \\
										& World-independent & Location can ignore world features \\
										& Mixed dependencies	& World-independent plus another \\ \midrule
					\textbf{Flow}		& Discrete		& Response occurs \textit{after} the user acts \\ 
										& Continuous 	& Response occurs \textit{while} the user acts  \\ \bottomrule
				\end{tabular}
			\end{footnotesize}
		\end{center}
	\end{table}
	\subsection{User-defined display interaction set}
	ToDo: Beschreibung + Tabelle set wie voice
	\subsection{Classification of Mid-Air Gestures}
	ToDo: Beschreibung Taxonomy wie voice
	ToDo: Tabelle Taxonomy
	\subsection{User-defined mid-air gestures set}
	ToDo: Beschreibung + Tabelle set wie voice
	
	\subsection{Comparing the Modalities}
	PLACEHOLDER Figure \ref{figure:ModalityRating}
	\begin{figure}
		\label{figure:ModalityRating}			
		\centering
		\includegraphics[width=\columnwidth]{MeanGesamt}
		\caption{Modalities rated by the participants, for each task on a scale from 1="not fitting at all" to 7="very fitting"}
		\Description{Modalities rated by the participants, for each task on a scale from 1="not fitting at all" to 7="very fitting"}
	\end{figure}
	
	\subsection{Qualitative Results}
	The qualitative data obtained through the semi-structured interview was analysed by splitting the participants opinions into six categories, obtained through an open-coding approach carried out by the three researchers individually. These are evaluation of the experiment, voice control, display control and mid-air gesture control, as well as possible mixtures of interaction modalities and further suggested ones. The participants mainly thought of the experiment as interesting and innovative. Participants stated it was "entertaining and interesting to test out new things I've never done before". Looking into general opinions on the different interaction modalities, the participants voice mainly favourable opinions about voice control. They stated voice control is "clear and unambiguous, like commanding". Display control was regarded as easy to control and intuitive, "it's the most universal and comfortable to use", while also classing it as time-consuming, stating it's "tedious, always having to hold a display, like a smart phone, in your hands". On the other hand, participants were not as convinced by mid-air gesture control, as is mirrored in the collected quantitative data. They stated it's "complicated to use" and "only intuitive if all gestures are the same", explaining that "gestures you use seldom, you completely forget after like a month". The surveillance aspect also valued into those opinions, with participants stating that "I don't want to be under constant surveillance, I wouldn't implement such an interaction technique", which also influenced their view on voice control, even though not as much, as participants specified that "voice control is not as bad as video monitoring, though it would be better if you could turn it off.". When asked about the possibilities of mixing two or three of the suggested interaction modalities in one smart home environment, participants were generally in favour of it, stating "combining like two, like voice and display, would be the most convenient". Participants were also asked about other interaction modalities different from voice, gesture and display control that come to their mind, resulting in interesting approaches for possible future work such as "I'd like haptic feedback, such as stomping once on the floor to activate my underfloor heating" or "I don't want to talk too much to my smart home, I want it to know my demands autonomously". 
	
	\section{Discussion}
	PLACEHOLDER
	
	\section{Future Work}
	The research offers a groundwork for future work in the surroundings of smart home development. Additionally, it also provides the base of producing even more insights on the examined interaction modalities through extending and improving the existing study. To achieve this, future work can build on the qualitative research that showed participants stating they mostly prefer display and voice control, with a possible combination of these two interaction modalities. Further work needs to focus on this approach, while also testing a combination of interaction modalities within a setting including more actual implemented parts of a smart home system. Although simulations offer research that is faster to develop and perform, they can not guarantee with an absolute certainty users would act the same way in a real smart home environment. To achieve a higher transferability and generalizability of the research, the number of participants and population to draw from needs to be significantly extended. 
	Leaving the approach to interact with user's short-time desires leads to us to future work centering around smart home-user interaction for long-time settings. Many of the participants stated their wish to interact with the smart home as little as possible. Instead, they preferred a scenario of setting their needs and demands at an early point in the implementation and rely on the smart home system working autonomous from that moment on. This could lead to a drastic change in participants' viewpoints on goodness, ease and social acceptance of interaction modalities. Performing a command only once compared to multiple times could lead to more lenient opinions on complex voice commands or mid-air gestures, or highly detailed structures of display commands.
	Safety issues mentioned in the qualitative research concerning the need to monitor users' in order to implement voice and mid-air gesture control can also lead to further research. In the time of highly demanded data security and privacy, smart homes can not pose as threats to this. Future work can focus on studies developing interaction modalities that are free from such concerns or improve existing modalities to no longer rely on constant user supervision. For this, it's necessary to collaborate with experts of other fields.
	
	\section{Conclusion}
	PLACEHOLDER
	
	
	%%
	%% The next two lines define the bibliography style to be used, and
	%% the bibliography file.
	\bibliographystyle{ACM-Reference-Format}
	\bibliography{AUE_Paper}
	
	%%
	%% If your work has an appendix, this is the place to put it.
\end{document}
\endinput
%%
%% End of file `sample-sigchi.tex'.